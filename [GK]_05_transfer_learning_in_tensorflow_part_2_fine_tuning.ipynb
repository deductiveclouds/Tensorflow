{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deductiveclouds/Tensorflow/blob/main/%5BGK%5D_05_transfer_learning_in_tensorflow_part_2_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!black [GK]_05_transfer_learning_in_tensorflow_part_2_fine_tuning.ipynb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "j98V1zh2om-2",
        "outputId": "c8ca1feb-d0e9-48fe-96dd-360c1574aa41"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nb_black==1.0.5 in /usr/local/lib/python3.10/dist-packages (1.0.5)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from nb_black==1.0.5) (7.34.0)\n",
            "Requirement already satisfied: black in /usr/local/lib/python3.10/dist-packages (from nb_black==1.0.5) (24.10.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black->nb_black==1.0.5) (8.1.7)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from black->nb_black==1.0.5) (1.0.0)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.10/dist-packages (from black->nb_black==1.0.5) (24.2)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from black->nb_black==1.0.5) (0.12.1)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black->nb_black==1.0.5) (4.3.6)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black->nb_black==1.0.5) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black->nb_black==1.0.5) (4.12.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->nb_black==1.0.5) (75.1.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython->nb_black==1.0.5) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->nb_black==1.0.5) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->nb_black==1.0.5) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->nb_black==1.0.5) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->nb_black==1.0.5) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->nb_black==1.0.5) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->nb_black==1.0.5) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->nb_black==1.0.5) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->nb_black==1.0.5) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->nb_black==1.0.5) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->nb_black==1.0.5) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->nb_black==1.0.5) (0.2.13)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UsageError: Line magic function `%nb_black` not found.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "            setTimeout(function() {\n",
              "                var nbb_cell_id = 24;\n",
              "                var nbb_unformatted_code = \"!pip install nb_black=='1.0.5'\\n%nb_black\";\n",
              "                var nbb_formatted_code = \"!pip install nb_black=='1.0.5'\\n%nb_black\";\n",
              "                var nbb_cells = Jupyter.notebook.get_cells();\n",
              "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
              "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
              "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
              "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
              "                        }\n",
              "                        break;\n",
              "                    }\n",
              "                }\n",
              "            }, 500);\n",
              "            "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gppZ2AB2kjN7"
      },
      "outputs": [],
      "source": [
        "# Preliminary setup for new runtime\n",
        "# !pip uninstall -y tensorflow\n",
        "# !pip install tensorflow=='2.15.0'\n",
        "# !pip freeze | grep tensorflow\n",
        "# !sudo apt install nvidia-utils-560"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fu4uE0_fnBVM"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "ez6pVd7nnRYC",
        "outputId": "f93acd96-cc33-4f26-9483-1b474f373c06"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "            setTimeout(function() {\n",
              "                var nbb_cell_id = 25;\n",
              "                var nbb_unformatted_code = \"# Get helper_functions.py from github\\n# !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\\n\\n# Import functions to use from above file\\nfrom helper_functions import (create_tensorboard_callback,plot_loss_curves, unzip_data,walk_through_dir,)\";\n",
              "                var nbb_formatted_code = \"# Get helper_functions.py from github\\n# !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\\n\\n# Import functions to use from above file\\nfrom helper_functions import (\\n    create_tensorboard_callback,\\n    plot_loss_curves,\\n    unzip_data,\\n    walk_through_dir,\\n)\";\n",
              "                var nbb_cells = Jupyter.notebook.get_cells();\n",
              "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
              "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
              "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
              "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
              "                        }\n",
              "                        break;\n",
              "                    }\n",
              "                }\n",
              "            }, 500);\n",
              "            "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Get helper_functions.py from github\n",
        "# !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
        "\n",
        "# Import functions to use from above file\n",
        "from helper_functions import (create_tensorboard_callback,plot_loss_curves, unzip_data,walk_through_dir,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xigp-pAWpWU1"
      },
      "outputs": [],
      "source": [
        "# # Get 10% of the data\n",
        "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
        "\n",
        "# # Unzip the data\n",
        "unzip_data(\"10_food_classes_10_percent.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wWkwp7yp1Ya"
      },
      "outputs": [],
      "source": [
        "# Walk through the directory for 10 percent\n",
        "walk_through_dir(\"10_food_classes_10_percent\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_YKZwQYqDrW"
      },
      "outputs": [],
      "source": [
        "# Create path names to directories for 10 percent\n",
        "train_dir = \"10_food_classes_10_percent/train/\"\n",
        "test_dir = \"10_food_classes_10_percent/test\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6czXEU1HqWKG"
      },
      "outputs": [],
      "source": [
        "# Create input data for 10 percent\n",
        "import tensorflow as tf\n",
        "\n",
        "IMAGE_SIZE = (224, 224, 3)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory=train_dir,\n",
        "    image_size=IMAGE_SIZE[:2],\n",
        "    label_mode=\"categorical\",\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "test_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory=test_dir, image_size=IMAGE_SIZE[:2], label_mode=\"categorical\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAapPMnfswgI"
      },
      "outputs": [],
      "source": [
        "# Check out the data\n",
        "train_data_10_percent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FMeHDbBs5lk"
      },
      "outputs": [],
      "source": [
        "# Check out the labels\n",
        "test_data_10_percent.class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ps0X_8Ups-rd"
      },
      "outputs": [],
      "source": [
        "# See contents of a batch of data\n",
        "import random\n",
        "\n",
        "batch_num = random.randint(0, len(train_data_10_percent))\n",
        "for index, items in enumerate(train_data_10_percent.take(batch_num)):\n",
        "    print(index, items[0].shape, items[1].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuA6XqowthW5"
      },
      "outputs": [],
      "source": [
        "# Create and fit the baseline model_0\n",
        "from os import name\n",
        "\n",
        "base_model = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(include_top=False)\n",
        "base_model.trainable = False\n",
        "\n",
        "inputs = tf.keras.layers.Input(shape=IMAGE_SIZE, name=\"input_layer\")\n",
        "x = base_model(inputs)\n",
        "\n",
        "x = tf.keras.layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer_2d\")(x)\n",
        "\n",
        "outputs = tf.keras.layers.Dense(units=10, activation=\"softmax\", name=\"output_layer\")(x)\n",
        "\n",
        "model_0 = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "model_0.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "history_model_0 = model_0.fit(\n",
        "    x=train_data_10_percent,\n",
        "    epochs=5,\n",
        "    steps_per_epoch=len(train_data_10_percent),\n",
        "    validation_data=test_data_10_percent,\n",
        "    validation_steps=len(test_data_10_percent),\n",
        "    callbacks=[\n",
        "        create_tensorboard_callback(\n",
        "            dir_name=\"transfer_learning\", experiment_name=\"model_0\"\n",
        "        )\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbKRZCMV2iqj"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUDgJYLZgM6u"
      },
      "outputs": [],
      "source": [
        "# Check based model layers\n",
        "for index, layer in enumerate(base_model.layers):\n",
        "    print(index, layer.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdsWNJu3hLP7"
      },
      "outputs": [],
      "source": [
        "base_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_I-BbkmohUDq"
      },
      "outputs": [],
      "source": [
        "model_0.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZlTpkXNvnHr"
      },
      "outputs": [],
      "source": [
        "plot_loss_curves(history_model_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JfnVOmT64a8"
      },
      "outputs": [],
      "source": [
        "# Create 1 percent of the data\n",
        "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_1_percent.zip\n",
        "\n",
        "unzip_data(\"10_food_classes_1_percent.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBDjHyzC7Tzs"
      },
      "outputs": [],
      "source": [
        "# Create path names to directories for 1 percent\n",
        "train_dir = \"10_food_classes_1_percent/train/\"\n",
        "test_dir = \"10_food_classes_1_percent/test/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzAaMK6P78ZM"
      },
      "outputs": [],
      "source": [
        "# Walk through the directory for 10 percent\n",
        "walk_through_dir(\"10_food_classes_1_percent\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDEDfXja8SBm"
      },
      "outputs": [],
      "source": [
        "# Create input data for 10 percent\n",
        "train_data_1_percent = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory=train_dir,\n",
        "    image_size=IMAGE_SIZE[:2],\n",
        "    label_mode=\"categorical\",\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "test_data_1_percent = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory=test_dir, image_size=IMAGE_SIZE[:2], label_mode=\"categorical\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEQgKnGYBOLQ"
      },
      "outputs": [],
      "source": [
        "# Create data augmentation layer\n",
        "data_augmentation = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.RandomFlip(mode=\"horizontal\"),\n",
        "        tf.keras.layers.RandomRotation(factor=0.2),\n",
        "        tf.keras.layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
        "        tf.keras.layers.RandomHeight(factor=0.2),\n",
        "        tf.keras.layers.RandomWidth(factor=0.2),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIn-gPtDGLOj"
      },
      "outputs": [],
      "source": [
        "# Create and fit the 1 percent with data augmentation model_1\n",
        "inputs = tf.keras.layers.Input(shape=IMAGE_SIZE, name=\"input_layer\")\n",
        "\n",
        "x = data_augmentation(inputs)\n",
        "\n",
        "base_model = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(include_top=False)\n",
        "base_model.trainable = False\n",
        "\n",
        "x = base_model(x, training=False)\n",
        "\n",
        "x = tf.keras.layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\n",
        "\n",
        "outputs = tf.keras.layers.Dense(units=10, activation=\"softmax\", name=\"output_layer\")(x)\n",
        "\n",
        "model_1 = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "model_1.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "history_model_1 = model_1.fit(\n",
        "    x=train_data_1_percent,\n",
        "    epochs=5,\n",
        "    steps_per_epoch=len(train_data_1_percent),\n",
        "    validation_data=test_data_1_percent,\n",
        "    validation_steps=(0.25 * len(test_data_1_percent)),\n",
        "    callbacks=[\n",
        "        create_tensorboard_callback(\n",
        "            dir_name=\"transfer_learning\", experiment_name=\"model_1\"\n",
        "        )\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ILtiM9q2oT-"
      },
      "outputs": [],
      "source": [
        "# Evaluate on the test data\n",
        "results_1_percent_data_aug = model_1.evaluate(test_data_1_percent)\n",
        "results_1_percent_data_aug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeoX3c92W5NT"
      },
      "outputs": [],
      "source": [
        "plot_loss_curves(history_model_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvZXUTWhitDI"
      },
      "outputs": [],
      "source": [
        "# Remove comment when starting a new runtime\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def create_base_model(\n",
        "    input_shape: tuple[int, int, int] = (224, 224, 3),\n",
        "    output_shape: int = 10,\n",
        "    learning_rate: float = 0.001,\n",
        "    training: bool = False,\n",
        ") -> tf.keras.Model:\n",
        "    base_model = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(\n",
        "        include_top=False\n",
        "    )\n",
        "    base_model.trainable = training\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape, name=\"input_layer\")\n",
        "    x = data_augmentation(inputs)\n",
        "    x = base_model(x, training=False)\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\n",
        "    outputs = tf.keras.layers.Dense(\n",
        "        units=output_shape, activation=\"softmax\", name=\"output_layer\"\n",
        "    )(x)\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    model.compile(\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EKt8hC4itDI"
      },
      "outputs": [],
      "source": [
        "model_2 = create_base_model()\n",
        "checkpoint_path = \"ten_percent_model_checkpoint_weights/checkpoint.ckpt\"\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    save_weights_only=True,\n",
        "    save_best_only=True,\n",
        "    save_freq=\"epoch\",\n",
        "    verbose=1,\n",
        ")\n",
        "initial_epochs = 5\n",
        "history_model_2 = model_2.fit(\n",
        "    x=train_data_10_percent,\n",
        "    epochs=initial_epochs,\n",
        "    validation_data=test_data_10_percent,\n",
        "    validation_steps=int(0.25 * len(test_data_10_percent)),\n",
        "    callbacks=[\n",
        "        create_tensorboard_callback(\"transfer_learning\", \"10_percent_data_aug\"),\n",
        "        checkpoint_callback,\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5Wxdd5vitDI"
      },
      "outputs": [],
      "source": [
        "results_10_percent_data_aug = model_2.evaluate(test_data_10_percent)\n",
        "results_10_percent_data_aug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIQO-_mMitDJ"
      },
      "outputs": [],
      "source": [
        "plot_loss_curves(history_model_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhsy3uTbitDK"
      },
      "outputs": [],
      "source": [
        "model_2.load_weights(checkpoint_path)\n",
        "loaded_weights_model_results = model_2.evaluate(test_data_10_percent)\n",
        "results_10_percent_data_aug == loaded_weights_model_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isqcPl15itDK"
      },
      "outputs": [],
      "source": [
        "# Remove comment when starting a new runtime\n",
        "import numpy as np\n",
        "\n",
        "np.isclose(results_10_percent_data_aug, loaded_weights_model_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EtkilPVitDK"
      },
      "outputs": [],
      "source": [
        "for layer_number, layer in enumerate(model_2.layers):\n",
        "    print(\n",
        "        f\"Layer: {layer_number}, Layer name: {layer.name}, Trainable: {layer.trainable}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYe94LUKitDK"
      },
      "outputs": [],
      "source": [
        "model_3 = model_2\n",
        "model_3_base_model = model_3.layers[2]\n",
        "model_3_base_model.name\n",
        "model_3_base_model.trainable = True\n",
        "for layer in model_3_base_model.layers[:-10]:\n",
        "    layer.trainable = False\n",
        "model_3.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "fine_tune_epochs = initial_epochs + 5\n",
        "history_model_3 = model_3.fit(\n",
        "    x=train_data_10_percent,\n",
        "    epochs=fine_tune_epochs,\n",
        "    steps_per_epoch=len(train_data_10_percent),\n",
        "    initial_epoch=history_model_2.epoch[-1],\n",
        "    validation_data=test_data_10_percent,\n",
        "    validation_steps=int(0.25 * len(test_data_10_percent)),\n",
        "    callbacks=[\n",
        "        create_tensorboard_callback(\"transfer_learning\", \"10_percent_fine_tune_last_10\")\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHbPiy65itDK"
      },
      "outputs": [],
      "source": [
        "# Remove comment when starting a new runtime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def compare_histories(original_history, new_history, initial_epochs=5):\n",
        "    acc = original_history.history[\"accuracy\"]\n",
        "    loss = original_history.history[\"loss\"]\n",
        "    val_acc = original_history.history[\"val_accuracy\"]\n",
        "    val_loss = original_history.history[\"val_loss\"]\n",
        "    total_acc = acc + new_history.history[\"accuracy\"]\n",
        "    total_loss = loss + new_history.history[\"loss\"]\n",
        "    total_val_acc = val_acc + new_history.history[\"val_accuracy\"]\n",
        "    total_val_loss = val_loss + new_history.history[\"val_loss\"]\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(total_acc, label=\"Training Accuracy\")\n",
        "    plt.plot(total_val_acc, label=\"Validation Accuracy\")\n",
        "    plt.plot(\n",
        "        [initial_epochs - 1, initial_epochs - 1], plt.ylim(), label=\"Start Fine Tuning\"\n",
        "    )\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.title(\"Training and Validation Accuracy\")\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(total_loss, label=\"Training Loss\")\n",
        "    plt.plot(total_val_loss, label=\"Validation Loss\")\n",
        "    plt.plot(\n",
        "        [initial_epochs - 1, initial_epochs - 1], plt.ylim(), label=\"Start Fine Tuning\"\n",
        "    )\n",
        "    plt.legend(loc=\"upper right\")\n",
        "    plt.title(\"Training and Validation Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zzp8hn9itDL"
      },
      "outputs": [],
      "source": [
        "compare_histories(original_history=history_model_2, new_history=history_model_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7thvByv-itDL"
      },
      "outputs": [],
      "source": [
        "# Remove comment when starting a new runtime\n",
        "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_all_data.zip\n",
        "\n",
        "unzip_data(\"10_food_classes_all_data.zip\")\n",
        "\n",
        "train_dir_full = \"10_food_classes_all_data/train/\"\n",
        "test_dir_full = \"10_food_classes_all_data/test/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWMK8EL1itDM"
      },
      "outputs": [],
      "source": [
        "walk_through_dir(\"10_food_classes_all_data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wV8w6FzwitDM"
      },
      "outputs": [],
      "source": [
        "train_data_full = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory=train_dir_full,\n",
        "    image_size=IMAGE_SIZE[:2],\n",
        "    label_mode=\"categorical\",\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "test_data_full = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory=test_dir_full,\n",
        "    image_size=IMAGE_SIZE[:2],\n",
        "    label_mode=\"categorical\",\n",
        "    batch_size=BATCH_SIZE,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcqoEZ9xitDM"
      },
      "outputs": [],
      "source": [
        "model_4 = create_base_model(learning_rate=0.0001)\n",
        "model_4.load_weights(checkpoint_path)\n",
        "model_4.evaluate(test_data_full)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrjVWSxeitDN"
      },
      "outputs": [],
      "source": [
        "model_4_base_model = model_4.layers[2]\n",
        "model_4_base_model.trainable = True\n",
        "for layer in model_4_base_model.layers[:-10]:\n",
        "    layer.trainable = False\n",
        "model_4.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "fine_tune_epochs = initial_epochs + 5\n",
        "history_model_4 = model_4.fit(\n",
        "    x=train_data_full,\n",
        "    epochs=fine_tune_epochs,\n",
        "    steps_per_epoch=len(train_data_full),\n",
        "    initial_epoch=history_model_2.epoch[-1],\n",
        "    validation_data=test_data_full,\n",
        "    validation_steps=int(0.25 * len(test_data_full)),\n",
        "    callbacks=[\n",
        "        create_tensorboard_callback(\n",
        "            \"transfer_learning\", \"full_10_classes_fine_tune_last_10\"\n",
        "        )\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fJmkM_3itDN"
      },
      "outputs": [],
      "source": [
        "results_fine_tune_full_data = model_4.evaluate(test_data_full)\n",
        "results_fine_tune_full_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0VYKlNkitDO"
      },
      "outputs": [],
      "source": [
        "compare_histories(history_model_2, history_model_4)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}