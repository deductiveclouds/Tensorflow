{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deductiveclouds/Tensorflow/blob/main/%5BGK%5D_05_transfer_learning_in_tensorflow_part_2_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gppZ2AB2kjN7"
      },
      "outputs": [],
      "source": [
        "# Preliminary setup for new runtime\n",
        "# !pip uninstall -y tensorflow\n",
        "# !pip install tensorflow=='2.15.0'\n",
        "# !pip freeze | grep tensorflow\n",
        "# !sudo apt install nvidia-utils-560"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fu4uE0_fnBVM"
      },
      "outputs": [],
      "source": [
        "# !nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ez6pVd7nnRYC"
      },
      "outputs": [],
      "source": [
        "# Get helper_functions.py from github\n",
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
        "\n",
        "# Import functions to use from above file\n",
        "from helper_functions import (\n",
        "    create_tensorboard_callback,\n",
        "    plot_loss_curves,\n",
        "    unzip_data,\n",
        "    walk_through_dir,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xigp-pAWpWU1"
      },
      "outputs": [],
      "source": [
        "# # Get 10% of the data\n",
        "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
        "\n",
        "# # Unzip the data\n",
        "unzip_data(\"10_food_classes_10_percent.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wWkwp7yp1Ya"
      },
      "outputs": [],
      "source": [
        "# Walk through the directory for 10 percent\n",
        "walk_through_dir(\"10_food_classes_10_percent\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_YKZwQYqDrW"
      },
      "outputs": [],
      "source": [
        "# Create path names to directories for 10 percent\n",
        "train_dir = \"10_food_classes_10_percent/train/\"\n",
        "test_dir = \"10_food_classes_10_percent/test\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6czXEU1HqWKG"
      },
      "outputs": [],
      "source": [
        "# Create input data for 10 percent\n",
        "import tensorflow as tf\n",
        "\n",
        "IMAGE_SIZE = (224, 224, 3)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory=train_dir,\n",
        "    image_size=IMAGE_SIZE[:2],\n",
        "    label_mode=\"categorical\",\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "test_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory=test_dir, image_size=IMAGE_SIZE[:2], label_mode=\"categorical\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAapPMnfswgI"
      },
      "outputs": [],
      "source": [
        "# Check out the data\n",
        "train_data_10_percent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FMeHDbBs5lk"
      },
      "outputs": [],
      "source": [
        "# Check out the labels\n",
        "test_data_10_percent.class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ps0X_8Ups-rd"
      },
      "outputs": [],
      "source": [
        "# See contents of a batch of data\n",
        "import random\n",
        "\n",
        "batch_num = random.randint(0, len(train_data_10_percent))\n",
        "for index, items in enumerate(train_data_10_percent.take(batch_num)):\n",
        "    print(index, items[0].shape, items[1].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuA6XqowthW5"
      },
      "outputs": [],
      "source": [
        "# Create and fit the baseline model_0\n",
        "from os import name\n",
        "\n",
        "base_model = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(include_top=False)\n",
        "base_model.trainable = False\n",
        "\n",
        "inputs = tf.keras.layers.Input(shape=IMAGE_SIZE, name=\"input_layer\")\n",
        "x = base_model(inputs)\n",
        "\n",
        "x = tf.keras.layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer_2d\")(x)\n",
        "\n",
        "outputs = tf.keras.layers.Dense(units=10, activation=\"softmax\", name=\"output_layer\")(x)\n",
        "\n",
        "model_0 = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "model_0.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "history_model_0 = model_0.fit(\n",
        "    x=train_data_10_percent,\n",
        "    epochs=5,\n",
        "    steps_per_epoch=len(train_data_10_percent),\n",
        "    validation_data=test_data_10_percent,\n",
        "    validation_steps=len(test_data_10_percent),\n",
        "    callbacks=[\n",
        "        create_tensorboard_callback(\n",
        "            dir_name=\"transfer_learning\", experiment_name=\"model_0\"\n",
        "        )\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbKRZCMV2iqj"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUDgJYLZgM6u"
      },
      "outputs": [],
      "source": [
        "# Check based model layers\n",
        "for index, layer in enumerate(base_model.layers):\n",
        "    print(index, layer.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdsWNJu3hLP7"
      },
      "outputs": [],
      "source": [
        "base_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_I-BbkmohUDq"
      },
      "outputs": [],
      "source": [
        "model_0.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZlTpkXNvnHr"
      },
      "outputs": [],
      "source": [
        "plot_loss_curves(history_model_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JfnVOmT64a8"
      },
      "outputs": [],
      "source": [
        "# Create 1 percent of the data\n",
        "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_1_percent.zip\n",
        "\n",
        "unzip_data(\"10_food_classes_1_percent.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBDjHyzC7Tzs"
      },
      "outputs": [],
      "source": [
        "# Create path names to directories for 1 percent\n",
        "train_dir = \"10_food_classes_1_percent/train/\"\n",
        "test_dir = \"10_food_classes_1_percent/test/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzAaMK6P78ZM"
      },
      "outputs": [],
      "source": [
        "# Walk through the directory for 10 percent\n",
        "walk_through_dir(\"10_food_classes_1_percent\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDEDfXja8SBm"
      },
      "outputs": [],
      "source": [
        "# Create input data for 10 percent\n",
        "train_data_1_percent = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory=train_dir,\n",
        "    image_size=IMAGE_SIZE[:2],\n",
        "    label_mode=\"categorical\",\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "test_data_1_percent = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory=test_dir, image_size=IMAGE_SIZE[:2], label_mode=\"categorical\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEQgKnGYBOLQ"
      },
      "outputs": [],
      "source": [
        "# Create data augmentation layer\n",
        "data_augmentation = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.RandomFlip(mode=\"horizontal\"),\n",
        "        tf.keras.layers.RandomRotation(factor=0.2),\n",
        "        tf.keras.layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
        "        tf.keras.layers.RandomHeight(factor=0.2),\n",
        "        tf.keras.layers.RandomWidth(factor=0.2),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIn-gPtDGLOj"
      },
      "outputs": [],
      "source": [
        "# Create and fit the 1 percent with data augmentation model_1\n",
        "inputs = tf.keras.layers.Input(shape=IMAGE_SIZE, name=\"input_layer\")\n",
        "\n",
        "x = data_augmentation(inputs)\n",
        "\n",
        "base_model = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(include_top=False)\n",
        "base_model.trainable = False\n",
        "\n",
        "x = base_model(x, training=False)\n",
        "\n",
        "x = tf.keras.layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\n",
        "\n",
        "outputs = tf.keras.layers.Dense(units=10, activation=\"softmax\", name=\"output_layer\")(x)\n",
        "\n",
        "model_1 = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "model_1.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "history_model_1 = model_1.fit(\n",
        "    x=train_data_1_percent,\n",
        "    epochs=5,\n",
        "    steps_per_epoch=len(train_data_1_percent),\n",
        "    validation_data=test_data_1_percent,\n",
        "    validation_steps=(0.25 * len(test_data_1_percent)),\n",
        "    callbacks=[\n",
        "        create_tensorboard_callback(\n",
        "            dir_name=\"transfer_learning\", experiment_name=\"model_1\"\n",
        "        )\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ILtiM9q2oT-"
      },
      "outputs": [],
      "source": [
        "# Evaluate on the test data\n",
        "results_1_percent_data_aug = model_1.evaluate(test_data_1_percent)\n",
        "results_1_percent_data_aug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeoX3c92W5NT"
      },
      "outputs": [],
      "source": [
        "plot_loss_curves(history_model_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvZXUTWhitDI"
      },
      "outputs": [],
      "source": [
        "# Remove comment when starting a new runtime\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def create_base_model(\n",
        "    input_shape: tuple[int, int, int] = (224, 224, 3),\n",
        "    output_shape: int = 10,\n",
        "    learning_rate: float = 0.001,\n",
        "    training: bool = False,\n",
        ") -> tf.keras.Model:\n",
        "    base_model = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(\n",
        "        include_top=False\n",
        "    )\n",
        "    base_model.trainable = training\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape, name=\"input_layer\")\n",
        "    x = data_augmentation(inputs)\n",
        "    x = base_model(x, training=False)\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\n",
        "    outputs = tf.keras.layers.Dense(\n",
        "        units=output_shape, activation=\"softmax\", name=\"output_layer\"\n",
        "    )(x)\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    model.compile(\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EKt8hC4itDI"
      },
      "outputs": [],
      "source": [
        "model_2 = create_base_model()\n",
        "checkpoint_path = \"ten_percent_model_checkpoint_weights/checkpoint.ckpt\"\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    save_weights_only=True,\n",
        "    save_best_only=True,\n",
        "    save_freq=\"epoch\",\n",
        "    verbose=1,\n",
        ")\n",
        "initial_epochs = 5\n",
        "history_model_2 = model_2.fit(\n",
        "    x=train_data_10_percent,\n",
        "    epochs=initial_epochs,\n",
        "    validation_data=test_data_10_percent,\n",
        "    validation_steps=int(0.25 * len(test_data_10_percent)),\n",
        "    callbacks=[\n",
        "        create_tensorboard_callback(\"transfer_learning\", \"10_percent_data_aug\"),\n",
        "        checkpoint_callback,\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5Wxdd5vitDI"
      },
      "outputs": [],
      "source": [
        "results_10_percent_data_aug = model_2.evaluate(test_data_10_percent)\n",
        "results_10_percent_data_aug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIQO-_mMitDJ"
      },
      "outputs": [],
      "source": [
        "plot_loss_curves(history_model_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhsy3uTbitDK"
      },
      "outputs": [],
      "source": [
        "model_2.load_weights(checkpoint_path)\n",
        "loaded_weights_model_results = model_2.evaluate(test_data_10_percent)\n",
        "results_10_percent_data_aug == loaded_weights_model_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isqcPl15itDK"
      },
      "outputs": [],
      "source": [
        "# Remove comment when starting a new runtime\n",
        "import numpy as np\n",
        "\n",
        "np.isclose(results_10_percent_data_aug, loaded_weights_model_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EtkilPVitDK"
      },
      "outputs": [],
      "source": [
        "for layer_number, layer in enumerate(model_2.layers):\n",
        "    print(\n",
        "        f\"Layer: {layer_number}, Layer name: {layer.name}, Trainable: {layer.trainable}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYe94LUKitDK"
      },
      "outputs": [],
      "source": [
        "model_3 = model_2\n",
        "model_3_base_model = model_3.layers[2]\n",
        "model_3_base_model.name\n",
        "model_3_base_model.trainable = True\n",
        "for layer in model_3_base_model.layers[:-10]:\n",
        "    layer.trainable = False\n",
        "model_3.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "fine_tune_epochs = initial_epochs + 5\n",
        "history_model_3 = model_3.fit(\n",
        "    x=train_data_10_percent,\n",
        "    epochs=fine_tune_epochs,\n",
        "    steps_per_epoch=len(train_data_10_percent),\n",
        "    initial_epoch=history_model_2.epoch[-1],\n",
        "    validation_data=test_data_10_percent,\n",
        "    validation_steps=int(0.25 * len(test_data_10_percent)),\n",
        "    callbacks=[\n",
        "        create_tensorboard_callback(\"transfer_learning\", \"10_percent_fine_tune_last_10\")\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHbPiy65itDK"
      },
      "outputs": [],
      "source": [
        "# Remove comment when starting a new runtime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def compare_histories(original_history, new_history, initial_epochs=5):\n",
        "    acc = original_history.history[\"accuracy\"]\n",
        "    loss = original_history.history[\"loss\"]\n",
        "    val_acc = original_history.history[\"val_accuracy\"]\n",
        "    val_loss = original_history.history[\"val_loss\"]\n",
        "    total_acc = acc + new_history.history[\"accuracy\"]\n",
        "    total_loss = loss + new_history.history[\"loss\"]\n",
        "    total_val_acc = val_acc + new_history.history[\"val_accuracy\"]\n",
        "    total_val_loss = val_loss + new_history.history[\"val_loss\"]\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(total_acc, label=\"Training Accuracy\")\n",
        "    plt.plot(total_val_acc, label=\"Validation Accuracy\")\n",
        "    plt.plot(\n",
        "        [initial_epochs - 1, initial_epochs - 1], plt.ylim(), label=\"Start Fine Tuning\"\n",
        "    )\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.title(\"Training and Validation Accuracy\")\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(total_loss, label=\"Training Loss\")\n",
        "    plt.plot(total_val_loss, label=\"Validation Loss\")\n",
        "    plt.plot(\n",
        "        [initial_epochs - 1, initial_epochs - 1], plt.ylim(), label=\"Start Fine Tuning\"\n",
        "    )\n",
        "    plt.legend(loc=\"upper right\")\n",
        "    plt.title(\"Training and Validation Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zzp8hn9itDL"
      },
      "outputs": [],
      "source": [
        "compare_histories(original_history=history_model_2, new_history=history_model_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7thvByv-itDL"
      },
      "outputs": [],
      "source": [
        "# Remove comment when starting a new runtime\n",
        "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_all_data.zip\n",
        "\n",
        "unzip_data(\"10_food_classes_all_data.zip\")\n",
        "\n",
        "train_dir_full = \"10_food_classes_all_data/train/\"\n",
        "test_dir_full = \"10_food_classes_all_data/test/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWMK8EL1itDM"
      },
      "outputs": [],
      "source": [
        "walk_through_dir(\"10_food_classes_all_data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wV8w6FzwitDM"
      },
      "outputs": [],
      "source": [
        "train_data_full = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory=train_dir_full,\n",
        "    image_size=IMAGE_SIZE[:2],\n",
        "    label_mode=\"categorical\",\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "test_data_full = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory=test_dir_full,\n",
        "    image_size=IMAGE_SIZE[:2],\n",
        "    label_mode=\"categorical\",\n",
        "    batch_size=BATCH_SIZE,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcqoEZ9xitDM"
      },
      "outputs": [],
      "source": [
        "model_4 = create_base_model(learning_rate=0.0001)\n",
        "model_4.load_weights(checkpoint_path)\n",
        "model_4.evaluate(test_data_full)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrjVWSxeitDN"
      },
      "outputs": [],
      "source": [
        "model_4_base_model = model_4.layers[2]\n",
        "model_4_base_model.trainable = True\n",
        "for layer in model_4_base_model.layers[:-10]:\n",
        "    layer.trainable = False\n",
        "model_4.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "fine_tune_epochs = initial_epochs + 5\n",
        "history_model_4 = model_4.fit(\n",
        "    x=train_data_full,\n",
        "    epochs=fine_tune_epochs,\n",
        "    steps_per_epoch=len(train_data_full),\n",
        "    initial_epoch=history_model_2.epoch[-1],\n",
        "    validation_data=test_data_full,\n",
        "    validation_steps=int(0.25 * len(test_data_full)),\n",
        "    callbacks=[\n",
        "        create_tensorboard_callback(\n",
        "            \"transfer_learning\", \"full_10_classes_fine_tune_last_10\"\n",
        "        )\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fJmkM_3itDN"
      },
      "outputs": [],
      "source": [
        "results_fine_tune_full_data = model_4.evaluate(test_data_full)\n",
        "results_fine_tune_full_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0VYKlNkitDO"
      },
      "outputs": [],
      "source": [
        "compare_histories(history_model_2, history_model_4)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}